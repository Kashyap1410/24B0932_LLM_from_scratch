{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c79dfbc",
   "metadata": {},
   "source": [
    "# Coding Attention Mechanisms\n",
    "\n",
    "This chapter covers\n",
    "- Exploring the reasons for using attention mechanisms in neural networks\n",
    "- Introducing a basic self-attention framework and progressing to an enhanced self-attention mechanism\n",
    "- Implementing a causal attention module that allows LLMs to generate one token at a time\n",
    "- Masking randomly selected attention weights with dropout to reduce overfitting\n",
    "- Stacking multiple causal attention modules into a multi-head attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff2f1a",
   "metadata": {},
   "source": [
    "## 3.3 Attending to different parts of the input with self-attention\n",
    "\n",
    "In this section, we implement a simplified variant of self-attention, free from any trainable weights.\n",
    "\n",
    "In self-attention, our goal is to calculate context vectors $z^{(i)}$ for each element $x^{(i)}$ in the input sequence. A context vector can be interpreted as an enriched embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da87998",
   "metadata": {},
   "source": [
    "Firsty, let us calculate the attention scores for a single input token. This is done by taking dot product of the key vectors of each token with the query vector of the token whose attention scores we are finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75af65e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "query = inputs[1]  # Taking the 2nd input token as the query vector\n",
    "\n",
    "attn_scores_2 = query @ inputs.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aa79e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "print(query @ inputs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c46d1f",
   "metadata": {},
   "source": [
    "Normalize the attention scores using softmax to get the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ee059c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4883a",
   "metadata": {},
   "source": [
    "Now, we compute the context vector, $z^{(2)}$ by multiplying attention weights to the input token embeddings and summing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d01bec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = torch.zeros(inputs[1].shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5431a85",
   "metadata": {},
   "source": [
    "Now, we are extending this computation to calculate attention weights and context vectors for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1c78916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5595b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "context_vecs = attn_weights @ inputs\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66953e",
   "metadata": {},
   "source": [
    "## 3.4: Attention Mechanism with trainable weights\n",
    "\n",
    "We will implement the self-attention mechanism step by step by introducing the three trainable weight matrices $W_q$, $W_k$, and $W_v$. These three matrices are used to project the embedded input tokens, $x^{(i)}$, into query, key, and value\n",
    "vectors. The query vector $q^{(2)}$ is obtained via matrix multiplication between the input $x^{(2)}$ and the weight matrix $W_q$. Similarly, we obtain the key and value vectors via matrix multiplication involving the weight matrices $W_k$ and $W_v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56d4392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6864, 1.0577, 1.1389])\n"
     ]
    }
   ],
   "source": [
    "d_in = d_out = inputs.shape[1]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Let us calculate the context vector for the 2nd input token \n",
    "query_2 = inputs[1] @ W_query\n",
    "keys = inputs @ W_key # Key vectors for all input tokens\n",
    "values = inputs @ W_value # Value vectors for all input tokens\n",
    "\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / (d_out**0.5), dim=0)\n",
    "context_vector_2 = attn_weights_2 @ values\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295109a3",
   "metadata": {},
   "source": [
    "Now, we generate context vectors for all of the input tokens by implementing a compact self-attention Python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44c78bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6692, 1.0276, 1.1106],\n",
      "        [0.6864, 1.0577, 1.1389],\n",
      "        [0.6860, 1.0570, 1.1383],\n",
      "        [0.6738, 1.0361, 1.1180],\n",
      "        [0.6711, 1.0307, 1.1139],\n",
      "        [0.6783, 1.0441, 1.1252]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b339cc5",
   "metadata": {},
   "source": [
    "We can streamline the implementation above using PyTorch's Linear layers, which are equivalent to a matrix multiplication if we disable the bias units.\n",
    "\n",
    "Another big advantage of using nn.Linear over our manual nn.Parameter(torch.rand(...) approach is that nn.Linear has a preferred weight initialization scheme, which leads to more stable model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc2c36ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4162, -0.4953,  0.0470],\n",
      "        [ 0.4154, -0.4957,  0.0476],\n",
      "        [ 0.4155, -0.4957,  0.0476],\n",
      "        [ 0.4173, -0.5006,  0.0483],\n",
      "        [ 0.4178, -0.4996,  0.0477],\n",
      "        [ 0.4166, -0.4996,  0.0483]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed69e3",
   "metadata": {},
   "source": [
    "## 3.5 Hiding future words with causal attention\n",
    "\n",
    "The causal aspect involves modifying the attention mechanism to prevent the model from accessing future information in the sequence, which is crucial for tasks like language modeling, where each word prediction should only depend on previous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b9f50bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1607, 0.1599, 0.1613, 0.1679, 0.1971, 0.1531],\n",
      "        [0.1606, 0.1579, 0.1590, 0.1721, 0.1907, 0.1597],\n",
      "        [0.1606, 0.1580, 0.1591, 0.1720, 0.1907, 0.1595],\n",
      "        [0.1637, 0.1618, 0.1623, 0.1704, 0.1767, 0.1650],\n",
      "        [0.1631, 0.1629, 0.1638, 0.1676, 0.1836, 0.1590],\n",
      "        [0.1631, 0.1602, 0.1607, 0.1722, 0.1778, 0.1660]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2382,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3270, -0.3568,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3218, -0.3506, -0.3384,    -inf,    -inf,    -inf],\n",
      "        [-0.1822, -0.2026, -0.1975, -0.1128,    -inf,    -inf],\n",
      "        [-0.1385, -0.1401, -0.1312, -0.0915,  0.0668,    -inf],\n",
      "        [-0.2439, -0.2754, -0.2699, -0.1499, -0.0948, -0.2134]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b2cedb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5043, 0.4957, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3362, 0.3307, 0.3330, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2487, 0.2458, 0.2465, 0.2589, 0.0000, 0.0000],\n",
      "        [0.1939, 0.1937, 0.1947, 0.1993, 0.2183, 0.0000],\n",
      "        [0.1631, 0.1602, 0.1607, 0.1722, 0.1778, 0.1660]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2176b7",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique in deep learning where random neurons are temporarily \"dropped\" (ignored) during training. This prevents the model from becoming too reliant on specific neurons, helping reduce overfitting. Dropout is only applied during training, not during inference. \n",
    "\n",
    "This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "232758b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0086, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6725, 0.0000, 0.6661, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4975, 0.4916, 0.0000, 0.5178, 0.0000, 0.0000],\n",
      "        [0.3879, 0.3875, 0.3895, 0.0000, 0.4367, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3214, 0.0000, 0.3556, 0.3320]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886ad86",
   "metadata": {},
   "source": [
    "Now, let us implement a compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cbf97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3326,  0.5659, -0.3132],\n",
      "         [ 0.3456,  0.5650, -0.2237],\n",
      "         [ 0.3440,  0.5604, -0.2000],\n",
      "         [ 0.3103,  0.4941, -0.1606],\n",
      "         [ 0.2430,  0.4287, -0.1643],\n",
      "         [ 0.2648,  0.4316, -0.1375]],\n",
      "\n",
      "        [[ 0.3326,  0.5659, -0.3132],\n",
      "         [ 0.3456,  0.5650, -0.2237],\n",
      "         [ 0.3440,  0.5604, -0.2000],\n",
      "         [ 0.3103,  0.4941, -0.1606],\n",
      "         [ 0.2430,  0.4287, -0.1643],\n",
      "         [ 0.2648,  0.4316, -0.1375]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, self.d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0) # Imitating the bacthes produced by our Dataloader in Chapter 2.ipynb\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d5830",
   "metadata": {},
   "source": [
    "## 3.6 Extending single-head attention to multi-head attention\n",
    "\n",
    "The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections. This allows the model to jointly attend to information from different representation subspaces at different positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78c19f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3326,  0.5659, -0.3132,  0.0752,  0.4566,  0.2729],\n",
      "         [ 0.3456,  0.5650, -0.2237,  0.0313,  0.5977,  0.3053],\n",
      "         [ 0.3440,  0.5604, -0.2000,  0.0178,  0.6413,  0.3138],\n",
      "         [ 0.3103,  0.4941, -0.1606,  0.0089,  0.5729,  0.2785],\n",
      "         [ 0.2430,  0.4287, -0.1643,  0.0071,  0.5566,  0.2514],\n",
      "         [ 0.2648,  0.4316, -0.1375,  0.0023,  0.5363,  0.2508]],\n",
      "\n",
      "        [[ 0.3326,  0.5659, -0.3132,  0.0752,  0.4566,  0.2729],\n",
      "         [ 0.3456,  0.5650, -0.2237,  0.0313,  0.5977,  0.3053],\n",
      "         [ 0.3440,  0.5604, -0.2000,  0.0178,  0.6413,  0.3138],\n",
      "         [ 0.3103,  0.4941, -0.1606,  0.0089,  0.5729,  0.2785],\n",
      "         [ 0.2430,  0.4287, -0.1643,  0.0071,  0.5566,  0.2514],\n",
      "         [ 0.2648,  0.4316, -0.1375,  0.0023,  0.5363,  0.2508]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList( [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)] )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 3\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30b8b2",
   "metadata": {},
   "source": [
    "While the above is an intuitive and fully functional implementation of multi-head attention (wrapping the single-head attention CausalAttention implementation from earlier), we can write a stand-alone class called MultiHeadAttention to achieve the same\n",
    "\n",
    "Instead, we create single W_query, W_key, and W_value weight matrices and then split those into individual matrices for each attention head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc7f756d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Optional Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
