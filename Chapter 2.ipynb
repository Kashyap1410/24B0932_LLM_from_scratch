{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2484729",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text Data \n",
    "\n",
    "This chapter covers\n",
    "- Preparing text for large language model training\n",
    "- Splitting text into word and subword tokens\n",
    "- Byte pair encoding as a more advanced way of tokenizing text\n",
    "- Sampling training examples with a sliding window approach\n",
    "- Converting tokens into vectors that feed into a large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21be6a1",
   "metadata": {},
   "source": [
    "## 2.1 Understanding word embeddings\n",
    "\n",
    "The concept of converting data into a vector format is often referred to as embeddings. Since our goal is to train GPT-like\n",
    "LLMs, which learn to generate text one word at a time, this chapter focuses on word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc44c86",
   "metadata": {},
   "source": [
    "## Tokenizing text\n",
    "\n",
    "This section covers how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM. These tokens are either individual words or special characters, including punctuation characters. The text we will tokenize for LLM training is a short story by Edith Wharton called 'The Verdict'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb24dbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"verdict.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    original_text = file.read()\n",
    "\n",
    "print(original_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c6691",
   "metadata": {},
   "source": [
    "Let us work on a small piece of sample text to build a simple tokenizer that splits on whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b8768d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67201c9",
   "metadata": {},
   "source": [
    "Let us include more special characters and remove the empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "351c16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e76de5",
   "metadata": {},
   "source": [
    "Let us apply this on our text material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eb49156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', original_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9dbd0",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into token IDs\n",
    "\n",
    "In this section, we will convert these tokens from a Python string to an integer representation to produce the so-called token IDs. This conversion is an intermediate step before converting the token IDs into embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f871e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83d227",
   "metadata": {},
   "source": [
    "Let's implement a complete tokenizer class in Python with an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f83e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b341d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "test_data = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(test_data)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f38ba",
   "metadata": {},
   "source": [
    "## 2.4 Adding special context tokens\n",
    "\n",
    "Some tokenizers use special tokens to help the LLM with additional context. Here, we add an <|unk|> token to represent new and unknown words that were not part of the training data and thus not part of the existing vocabulary and <|endoftext|> token that we can use to separate two unrelated text sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6144699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb52f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b979147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff002124",
   "metadata": {},
   "source": [
    "## 2.5 Byte Pair Encoding\n",
    "\n",
    "This section covers a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE). The BPE\n",
    "tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d365b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(text_ids)\n",
    "print(tokenizer.decode(text_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c64094",
   "metadata": {},
   "source": [
    "Handling random words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "229872fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "example = \"Akwirw ier\"\n",
    "example_ids = tokenizer.encode(example)\n",
    "print(example_ids)\n",
    "print(tokenizer.decode(example_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541034f",
   "metadata": {},
   "source": [
    "## 2.6: Data Sampling with a sliding window\n",
    "\n",
    "The next step before we can finally create the embeddings for the LLM is to generate the input-target pairs required for training an LLM.\n",
    "In this section we implement a data loader that fetches the input-target pairs depicted in Figure 2.12 from the training dataset using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b40cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"verdict.txt\", 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4c7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
