{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2484729",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text Data \n",
    "\n",
    "This chapter covers\n",
    "- Preparing text for large language model training\n",
    "- Splitting text into word and subword tokens\n",
    "- Byte pair encoding as a more advanced way of tokenizing text\n",
    "- Sampling training examples with a sliding window approach\n",
    "- Converting tokens into vectors that feed into a large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21be6a1",
   "metadata": {},
   "source": [
    "## 2.1 Understanding word embeddings\n",
    "\n",
    "The concept of converting data into a vector format is often referred to as embeddings. Since our goal is to train GPT-like\n",
    "LLMs, which learn to generate text one word at a time, this chapter focuses on word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc44c86",
   "metadata": {},
   "source": [
    "## Tokenizing text\n",
    "\n",
    "This section covers how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM. These tokens are either individual words or special characters, including punctuation characters. The text we will tokenize for LLM training is a short story by Edith Wharton called 'The Verdict'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb24dbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"verdict.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    original_text = file.read()\n",
    "\n",
    "print(original_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c6691",
   "metadata": {},
   "source": [
    "Let us work on a small piece of sample text to build a simple tokenizer that splits on whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9b8768d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67201c9",
   "metadata": {},
   "source": [
    "Let us include more special characters and remove the empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "351c16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e76de5",
   "metadata": {},
   "source": [
    "Let us apply this on our text material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4eb49156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', original_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9dbd0",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into token IDs\n",
    "\n",
    "In this section, we will convert these tokens from a Python string to an integer representation to produce the so-called token IDs. This conversion is an intermediate step before converting the token IDs into embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "88f871e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83d227",
   "metadata": {},
   "source": [
    "Let's implement a complete tokenizer class in Python with an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8f83e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5b341d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "test_data = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(test_data)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f38ba",
   "metadata": {},
   "source": [
    "## 2.4 Adding special context tokens\n",
    "\n",
    "Some tokenizers use special tokens to help the LLM with additional context. Here, we add an <|unk|> token to represent new and unknown words that were not part of the training data and thus not part of the existing vocabulary and <|endoftext|> token that we can use to separate two unrelated text sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6144699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb52f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b979147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff002124",
   "metadata": {},
   "source": [
    "## 2.5 Byte Pair Encoding\n",
    "\n",
    "This section covers a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE). The BPE\n",
    "tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6d365b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(text_ids)\n",
    "print(tokenizer.decode(text_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c64094",
   "metadata": {},
   "source": [
    "Handling random words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "229872fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "example = \"Akwirw ier\"\n",
    "example_ids = tokenizer.encode(example)\n",
    "print(example_ids)\n",
    "print(tokenizer.decode(example_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541034f",
   "metadata": {},
   "source": [
    "## 2.6: Data Sampling with a sliding window\n",
    "\n",
    "The next step before we can finally create the embeddings for the LLM is to generate the input-target pairs required for training an LLM.\n",
    "In this section we implement a data loader that fetches the input-target pairs depicted in Figure 2.12 from the training dataset using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ce4c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42679996",
   "metadata": {},
   "source": [
    "The GPTDatasetV1 class defines how individual rows are fetched from the dataset, where each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor. The target_chunk tensor contains the corresponding targets. \n",
    "The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20c60c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "04b3facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5560e81d",
   "metadata": {},
   "source": [
    "We can also create batched outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1fcbd31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688937bf",
   "metadata": {},
   "source": [
    "## 2.7 Creating token embeddings\n",
    "\n",
    "The last step for preparing the input text for LLM training is to convert the token IDs into embedding vectors. \n",
    "\n",
    "For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f787282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426b8fc",
   "metadata": {},
   "source": [
    "To convert a token with id 3 into a 3-dimensional vector, we do the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d6dd2691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848765c5",
   "metadata": {},
   "source": [
    "Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c7251f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031afe2",
   "metadata": {},
   "source": [
    "An embedding layer is essentially a look-up operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cb9a9",
   "metadata": {},
   "source": [
    "## 2.8 Encoding word positions\n",
    "\n",
    "We will add a small modification to these embedding vectors to encode positional information about a token within a text.\n",
    "\n",
    "Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb9283",
   "metadata": {},
   "source": [
    "We now consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector\n",
    "representation. Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1a86a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a0881d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5930333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Convert the input token IDs into the 256-dimension embedding space.\n",
    "token_embeddings = token_embedding_layer(inputs) \n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# Uncomment & execute the following line to see how the embeddings look like\n",
    "# print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff836f6",
   "metadata": {},
   "source": [
    "Now, let us create the positional embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8c35d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c05b2",
   "metadata": {},
   "source": [
    "To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a6f662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(input_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
